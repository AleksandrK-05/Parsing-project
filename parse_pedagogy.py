import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin

def parse_pedsovet_articles():
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ä—Ç–æ—á–µ–∫ —Å—Ç–∞—Ç–µ–π —Å —Å–∞–π—Ç–∞ pedsovet.org
    """
    url = "https://pedsovet.org/"
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        articles_data = []
        
        # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Å—Ç–∞—Ç–µ–π
        print("üîç –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Å—Ç–∞—Ç–µ–π...")
        cards = soup.find_all('div', class_=lambda x: x and any(word in str(x) for word in ['card', 'item', 'news', 'article', 'post']))
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫, –µ—Å–ª–∏ –ø–µ—Ä–≤—ã–π –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
        if not cards:
            cards = soup.select('div[class*="card"], div[class*="item"], div[class*="news"]')
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫: {len(cards)}")
        
        for i, card in enumerate(cards, 1):
            try:
                # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_elem = card.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'a', 'span', 'div'])
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                if not title or len(title) < 5:
                    continue
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É
                link_elem = card.find('a', href=True)
                if link_elem:
                    link = link_elem['href']
                    if link.startswith('/'):
                        link = urljoin(url, link)
                else:
                    link = "–°—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
                articles_data.append({
                    'id': i,
                    'title': title,
                    'link': link
                })
                
                print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∞ {i}: {title[:50]}...")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ {i}: {e}")
                continue
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        print("\n" + "="*60)
        print("üéâ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–ê–†–°–ò–ù–ì–ê:")
        print("="*60)
        
        for article in articles_data:
            print(f"üìñ {article['title']}")
            print(f"üîó {article['link']}")
            print()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        with open('pedsovet_articles.json', 'w', encoding='utf-8') as f:
            json.dump(articles_data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: pedsovet_articles.json")
        print(f"üìà –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(articles_data)}")
        
        return articles_data
        
    except requests.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return []
    except Exception as e:
        print(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞: {e}")
        return []

if __name__ == "__main__":
    parse_pedsovet_articles()